{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hydra\n",
    "\n",
    "from train import train\n",
    "from dataset import MyDataset, CocoImages\n",
    "from models.base_model import MyModel\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import main_utils, train_utils#, data_loader\n",
    "from utils.train_logger import TrainLogger\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import os\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.types import Scores, Metrics\n",
    "from utils.train_utils import TrainParams\n",
    "from utils.train_logger import TrainLogger\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "print(\"current dict = \", os.getcwd())\n",
    "\"\"\"\n",
    "Run the code following a given configuration\n",
    ":param cfg: configuration file retrieved from hydra framework\n",
    "\"\"\"\n",
    "# # main_utils.init(cfg)\n",
    "# logger = TrainLogger(exp_name_prefix=cfg['main']['experiment_name_prefix'], logs_dir=cfg['main']['paths']['logs'])\n",
    "# print(logger)\n",
    "# logger.write(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "# # Set seed for results reproduction\n",
    "# main_utils.set_seed(cfg['main']['seed'])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = MyDataset(image_path='../../../datashare/train2014',\n",
    "                          questions_path='../../../datashare/v2_OpenEnded_mscoco_train2014_questions.json',\n",
    "                          answers_path='../../../datashare/v2_mscoco_train2014_annotations.json',\n",
    "                          train=True,\n",
    "                          answerable_only = False\n",
    "                         )\n",
    "# val_dataset = MyDataset(image_path='../../../datashare/val2014',\n",
    "#                           questions_path='../../../datashare/v2_OpenEnded_mscoco_val2014_questions.json',\n",
    "#                           answers_path='../../../datashare/v2_mscoco_val2014_annotations.json',\n",
    "#                           train=False,\n",
    "#                           answerable_only = False\n",
    "#                          )\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, 128, shuffle=True,\n",
    "                          num_workers=8)\n",
    "# eval_loader = DataLoader(val_dataset, 128, shuffle=False,\n",
    "#                          num_workers=8)\n",
    " \n",
    "\n",
    "\n",
    "# Init model\n",
    "image_in_size_input = ((3,224,224))\n",
    "img_encoder_channels_input = [32, 128, 512, 1024]\n",
    "# model = MyModel(image_in_size=image_in_size_input,\n",
    "#     img_encoder_out_classes=1024,\n",
    "#     img_encoder_channels=img_encoder_channels_input,\n",
    "#     img_encoder_batchnorm=True,\n",
    "#     img_encoder_dropout=0.5,\n",
    "#     text_embedding_tokens=15193,\n",
    "#     text_embedding_features=100,\n",
    "#     text_lstm_features=512,\n",
    "#     text_dropout=0.5,\n",
    "#     attention_mid_features=128,\n",
    "#     attention_glimpses=2,\n",
    "#     attention_dropout=0.5,\n",
    "#     classifier_dropout=0.5,\n",
    "#     classifier_mid_features=128,\n",
    "#     classifier_out_classes=2410,\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "# if cfg['main']['parallel']:\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "# torch.cuda.empty_cache()\n",
    "# model = model.cuda()\n",
    "\n",
    "# logger.write(main_utils.get_model_string(model))\n",
    "\n",
    "# Run model\n",
    "# train_params = train_utils.get_train_params(cfg)\n",
    "\n",
    "# Report metrics and hyper parameters to tensorboard\n",
    "for img, ans, ques, _, q_len in tqdm(train_loader):\n",
    "    if torch.cuda.is_available():\n",
    "        img = img.cuda()\n",
    "        ans = ans.cuda()\n",
    "        ques = ques.cuda()\n",
    "        q_len = q_len.cuda()\n",
    "\n",
    "# metrics = train(model, train_loader, eval_loader, train_params, logger)\n",
    "# hyper_parameters = main_utils.get_flatten_dict(cfg['train'])\n",
    "\n",
    "# logger.report_metrics_hyper_params(hyper_parameters, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, ans, ques, _, q_len in train_loader:\n",
    "    if torch.cuda.is_available():\n",
    "        img = img.cuda()\n",
    "        ans = ans.cuda()\n",
    "        ques = ques.cuda()\n",
    "        q_len = q_len.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matan_env",
   "language": "python",
   "name": "matan_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
