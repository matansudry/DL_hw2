{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hydra\n",
    "\n",
    "from train import train\n",
    "from dataset_temp import MyDataset\n",
    "from models.base_model import MyModel\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import main_utils, train_utils#, data_loader\n",
    "from utils.train_logger import TrainLogger\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import h5py\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.types import Scores, Metrics\n",
    "from utils.train_utils import TrainParams\n",
    "from utils.train_logger import TrainLogger\n",
    "\n",
    "from abc import ABCMeta\n",
    "# from nets.fc import FCNet\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import itertools as it\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.3930e-01, 6.1816e-01, 8.6801e-01,  ..., 2.0418e-01, 8.1415e-01,\n",
      "         3.6551e-01],\n",
      "        [8.0120e-01, 3.5179e-02, 3.6642e-01,  ..., 1.8765e-01, 1.9873e-01,\n",
      "         3.7349e-01],\n",
      "        [7.4810e-01, 4.5722e-01, 7.5049e-01,  ..., 5.7357e-01, 2.0105e-04,\n",
      "         8.0222e-01],\n",
      "        ...,\n",
      "        [3.6611e-01, 8.6023e-01, 6.0960e-01,  ..., 8.7644e-01, 1.1395e-01,\n",
      "         1.3263e-01],\n",
      "        [1.0315e-01, 1.9660e-01, 8.0521e-01,  ..., 2.3006e-01, 2.0783e-01,\n",
      "         7.6822e-01],\n",
      "        [9.5591e-01, 5.6640e-01, 9.6652e-01,  ..., 7.8752e-01, 6.9278e-01,\n",
      "         5.0014e-02]])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.9821, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "ans = torch.rand(64,20)\n",
    "print(ans)\n",
    "zeros = torch.zeros(ans.shape[1])\n",
    "max_values, predicted_index = ans.max(dim=1, keepdim=True)\n",
    "for i in range(ans.shape[0]):\n",
    "    ans[i] = torch.where(ans[i]==max_values[i], ans[i], zeros)\n",
    "print(ans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Example for a simple model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#POOLINGS = {\"avg\": nn.AvgPool2d, \"max\": nn.MaxPool2d}\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        channels: list,\n",
    "        kernel_sizes: list,\n",
    "        batchnorm=False,\n",
    "        dropout=0.0,\n",
    "        activation_type: str = \"relu\",\n",
    "        activation_params: dict = {},\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        assert channels and kernel_sizes\n",
    "        assert len(channels) == len(kernel_sizes)\n",
    "        assert all(map(lambda x: x % 2 == 1, kernel_sizes))\n",
    "\n",
    "        \"\"\"if activation_type not in ACTIVATIONS:\n",
    "            raise ValueError(\"Unsupported activation type\")\"\"\"\n",
    "\n",
    "        self.main_path, self.shortcut_path = None, None\n",
    "        main_layers = []\n",
    "        shortcut_layers = []\n",
    "\n",
    "        # - extract number of conv layers\n",
    "        N = len(channels)\n",
    "\n",
    "        # - first conv layer \n",
    "        main_layers.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                channels[0],\n",
    "                kernel_size= kernel_sizes[0],\n",
    "                padding=(int((kernel_sizes[0]-1)/2),\n",
    "                int((kernel_sizes[0]-1)/2)), bias=True))\n",
    "        if dropout !=0:\n",
    "            main_layers.append(torch.nn.Dropout2d(p=dropout))\n",
    "        if batchnorm == True:\n",
    "            main_layers.append(torch.nn.BatchNorm2d(channels[0], eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        main_layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        #middle layers\n",
    "        for i in range(1,N-1):\n",
    "            main_layers.append(\n",
    "                nn.Conv2d(\n",
    "                    channels[i-1],\n",
    "                    channels[i],\n",
    "                    kernel_size= kernel_sizes[i],\n",
    "                    padding=(int((kernel_sizes[i]-1)/2),\n",
    "                    int((kernel_sizes[i]-1)/2)), bias=True))\n",
    "            if dropout !=0:\n",
    "                main_layers.append(torch.nn.Dropout2d(p=dropout))\n",
    "            if batchnorm == True:\n",
    "                main_layers.append(torch.nn.BatchNorm2d(channels[i], eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "            if (i%2 == 1):\n",
    "                main_layers.append(nn.ReLU(inplace=True))\n",
    "        if N > 1:\n",
    "            main_layers.append(\n",
    "                nn.Conv2d(\n",
    "                    channels[N-2],\n",
    "                    channels[N-1],\n",
    "                    kernel_size= kernel_sizes[N-1],\n",
    "                    padding=(int((kernel_sizes[N-1]-1)/2),\n",
    "                    int((kernel_sizes[N-1]-1)/2)), bias=True))\n",
    "        if (in_channels != channels[N-1]):\n",
    "            shortcut_layers.append(nn.Conv2d (in_channels, channels[N-1], kernel_size= 1, bias=False))\n",
    "\n",
    "        self.main_path = nn.Sequential(*main_layers)\n",
    "        self.shortcut_path = nn.Sequential(*shortcut_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.main_path(x)\n",
    "        out = out + self.shortcut_path(x)\n",
    "        relu = torch.nn.ReLU()\n",
    "        out = relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_size,\n",
    "        channels,\n",
    "        pool_every,\n",
    "#         hidden_dims,\n",
    "        activation_type: str = \"relu\",\n",
    "        activation_params: dict = {},\n",
    "        pooling_type: str = \"max\",\n",
    "        pooling_params: dict = {},\n",
    "        batchnorm=False,\n",
    "        dropout=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        See arguments of ConvClassifier & ResidualBlock.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.batchnorm = batchnorm\n",
    "        self.dropout = dropout\n",
    "        self.conv_params=dict(kernel_size=3, stride=1, padding=1)\n",
    "        self.in_size = in_size\n",
    "        self.channels = channels\n",
    "        self.pool_every = pool_every\n",
    "        self.activation_type = activation_type\n",
    "        self.activation_params = activation_params\n",
    "        self.pooling_type = pooling_type\n",
    "        self.pooling_params = pooling_params\n",
    "        self.feature_extractor = self._make_feature_extractor()\n",
    "        \n",
    "\n",
    "\n",
    "    def _make_feature_extractor(self):\n",
    "        in_channels, in_h, in_w, = tuple(self.in_size)\n",
    "        layers = []\n",
    "        self.output_dim = 2048\n",
    "        # - extract number of conv layers\n",
    "        N = len(self.channels)\n",
    "        \n",
    "        #1st layer\n",
    "        temp_in_channels = in_channels\n",
    "        temp_channels = []\n",
    "        temp_kernel_sizes = []\n",
    "        \n",
    "        layers.append(nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False))\n",
    "        layers.append(torch.nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False))\n",
    "        \n",
    "        #middle layers\n",
    "        for i in range(1,N):\n",
    "            temp_channels.append(self.channels[i-1])\n",
    "            temp_kernel_sizes.append(3)\n",
    "            if ((i % self.pool_every)==0 and i!=0):\n",
    "                layers.append(\n",
    "                    ResidualBlock(\n",
    "                        in_channels=temp_in_channels,\n",
    "                        channels=temp_channels,\n",
    "                        kernel_sizes=temp_kernel_sizes,\n",
    "                        batchnorm= self.batchnorm,\n",
    "                        dropout= self.dropout,\n",
    "                        activation_type= self.activation_type))\n",
    "                temp_in_channels = self.channels[i-1]\n",
    "                temp_channels = []\n",
    "                temp_kernel_sizes = []\n",
    "                #layers.append(nn.AvgPool2d(self.pooling_params['kernel_size']))\n",
    "        temp_channels.append(self.channels[N-1])\n",
    "        temp_kernel_sizes.append(3)\n",
    "        layers.append(ResidualBlock(\n",
    "                in_channels=temp_in_channels,\n",
    "                channels=temp_channels,\n",
    "                kernel_sizes=temp_kernel_sizes,\n",
    "                batchnorm= self.batchnorm,\n",
    "                dropout= self.dropout,\n",
    "                activation_type= self.activation_type))\n",
    "        if ((N % self.pool_every)==0):\n",
    "#             layers.append(nn.AvgPool2d(self.pooling_params['kernel_size']))\n",
    "                layers.append(torch.nn.AdaptiveAvgPool2d(output_size=(1, 1))) ########################################################\n",
    "        layers.append(nn.Linear(1000, self.output_dim, bias=True))\n",
    "        # add to go to 1x1\n",
    "        #layers.append(nn.AvgPool2d(2))\n",
    "        #layers.append(nn.AvgPool2d(15))\n",
    "        seq = nn.Sequential(*layers)\n",
    "        return seq\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.feature_extractor(x)\n",
    "        batch_size = out.shape[0]\n",
    "        out = out.view(batch_size,-1,1,1)\n",
    "        #shortcut = self.shortcut_path(x)\n",
    "        #print(\"shortcut.shape = \", shortcut.shape)\n",
    "        #print(\"out.shape = \", out.shape)\n",
    "        #out = out + shortcut\n",
    "        #out = out.view(batch_size,-1,1,1)\n",
    "        #relu = torch.nn.ReLU()\n",
    "        #out = relu(out)\n",
    "        return out \n",
    "\n",
    "    \n",
    "class TextProcessor(nn.Module):\n",
    "    def __init__(self, embedding_tokens, embedding_features, lstm_features, drop=0.0):\n",
    "        super(TextProcessor, self).__init__()\n",
    "        self.embedding = nn.Embedding(embedding_tokens, embedding_features, padding_idx=0)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lstm = nn.LSTM(input_size=embedding_features,\n",
    "                            hidden_size=lstm_features,\n",
    "                            num_layers=1)\n",
    "        self.features = lstm_features\n",
    "\n",
    "        self._init_lstm(self.lstm.weight_ih_l0)\n",
    "        self._init_lstm(self.lstm.weight_hh_l0)\n",
    "        self.lstm.bias_ih_l0.data.zero_()\n",
    "        self.lstm.bias_hh_l0.data.zero_()\n",
    "\n",
    "        init.xavier_uniform(self.embedding.weight)\n",
    "\n",
    "    def _init_lstm(self, weight):\n",
    "        for w in weight.chunk(4, 0):\n",
    "            init.xavier_uniform(w)\n",
    "\n",
    "    def forward(self, ques, q_len):\n",
    "        embedded = self.embedding(ques)\n",
    "        tanhed = self.tanh(self.drop(embedded))\n",
    "        packed = pack_padded_sequence(tanhed, q_len, batch_first=True, enforce_sorted=False)\n",
    "        _, (_, c) = self.lstm(packed)\n",
    "        return c.squeeze(0)\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, v_features, q_features, mid_features, glimpses, drop=0.0):\n",
    "        super(Attention, self).__init__()\n",
    "        self.v_conv = nn.Conv2d(v_features, mid_features, 1, bias=False)  # let self.lin take care of bias\n",
    "        self.q_lin = nn.Linear(q_features, mid_features)\n",
    "        self.x_conv = nn.Conv2d(mid_features, glimpses, 1)\n",
    "\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, img, ques):\n",
    "        ques = self.q_lin(self.drop(ques))\n",
    "        img = self.v_conv(self.drop(img))\n",
    "        ques = tile_2d_over_nd(ques, img)\n",
    "        x = self.relu(img + ques)\n",
    "        x = self.x_conv(self.drop(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def apply_attention(input, attention):\n",
    "    \"\"\" Apply any number of attention maps over the input. \"\"\"\n",
    "    n, c = input.size()[:2]\n",
    "    glimpses = attention.size(1)\n",
    "\n",
    "    # flatten the spatial dims into the third dim, since we don't need to care about how they are arranged\n",
    "    input = input.view(n, 1, c, -1) # [n, 1, c, s]\n",
    "    attention = attention.view(n, glimpses, -1)\n",
    "    attention = F.softmax(attention, dim=-1).unsqueeze(2) # [n, g, 1, s]\n",
    "    weighted = attention * input # [n, g, v, s]\n",
    "    weighted_mean = weighted.sum(dim=-1) # [n, g, v]\n",
    "    return weighted_mean.view(n, -1)\n",
    "\n",
    "\n",
    "def tile_2d_over_nd(feature_vector, feature_map):\n",
    "    \"\"\" Repeat the same feature vector over all spatial positions of a given feature map.\n",
    "        The feature vector should have the same batch size and number of features as the feature map.\n",
    "    \"\"\"\n",
    "    n, c = feature_vector.size()\n",
    "    spatial_size = feature_map.dim() - 2\n",
    "    tiled = feature_vector.view(n, c, *([1] * spatial_size)).expand_as(feature_map)\n",
    "    return tiled    \n",
    "\n",
    "class ImageNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(ImageNet, self).__init__()\n",
    "        from torchvision import models\n",
    "        self.model = models.resnet34(pretrained=False)\n",
    "        self.fc = nn.Linear(1000, output_dim, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, image_tensor):\n",
    "        x = self.model(image_tensor)\n",
    "        x = self.fc(x)\n",
    "        return F.normalize(x, dim=1, p=2)\n",
    "\n",
    "class MyModel(nn.Module, metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    Example for a simple model\n",
    "    \"\"\"\n",
    "    #def __init__(self,):# input_dim: int = 50, num_hid: int = 256, output_dim: int = 2, dropout: float = 0.2):\n",
    "    def __init__(\n",
    "        self, image_in_size=((3,224,224)), img_encoder_out_classes=1024, img_encoder_channels=[32, 128, 512, 1024],\n",
    "        img_encoder_batchnorm=True, img_encoder_dropout=0.5, text_embedding_tokens=15193, text_embedding_features=100,\n",
    "        text_lstm_features=512, text_dropout=0.5, attention_mid_features=128, attention_glimpses=2, attention_dropout=0.5,\n",
    "        classifier_dropout=0.5,  classifier_mid_features=128,classifier_out_classes=2410\n",
    "        ):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.image_in_size=image_in_size\n",
    "        self.img_encoder_out_classes=img_encoder_out_classes\n",
    "        self.img_encoder_channels=img_encoder_channels\n",
    "        self.img_encoder_batchnorm=img_encoder_batchnorm\n",
    "        self.img_encoder_dropout=img_encoder_dropout\n",
    "        self.text_embedding_tokens=text_embedding_tokens\n",
    "        self.text_embedding_features=text_embedding_features\n",
    "        self.text_lstm_features=text_lstm_features\n",
    "        self.text_dropout=text_dropout\n",
    "        self.attention_mid_features=attention_mid_features\n",
    "        self.attention_glimpses=attention_glimpses\n",
    "        self.attention_dropout=attention_dropout\n",
    "        self.classifier_dropout=classifier_dropout \n",
    "        self.classifier_mid_features=classifier_mid_features\n",
    "        self.classifier_out_classes=classifier_out_classes\n",
    "\n",
    "        self.img_encoder = ResNetClassifier(\n",
    "            in_size=image_in_size,\n",
    "            channels=img_encoder_channels,\n",
    "            pool_every=8,\n",
    "            activation_type='relu',\n",
    "            activation_params=dict(),\n",
    "            pooling_type='avg',\n",
    "            pooling_params=dict(kernel_size=3),\n",
    "            batchnorm=img_encoder_batchnorm,\n",
    "            dropout=img_encoder_dropout,\n",
    "        )\n",
    "        self.text = TextProcessor(\n",
    "            embedding_tokens=text_embedding_tokens,\n",
    "            embedding_features=text_embedding_features, #300,\n",
    "            lstm_features=text_lstm_features,\n",
    "            drop=text_dropout,\n",
    "        )\n",
    "        \n",
    "        self.attention = Attention(\n",
    "            v_features=img_encoder_out_classes,#2048,\n",
    "            q_features=text_lstm_features,\n",
    "            mid_features=attention_mid_features,\n",
    "            glimpses=attention_glimpses,\n",
    "            drop=attention_dropout,\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(2 * img_encoder_out_classes + text_lstm_features, classifier_mid_features), #(2*2048+1024,256)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(classifier_mid_features, classifier_out_classes),\n",
    "        )\n",
    "#         self.img_encoder = ConvClassifier(**test_params)\n",
    "        #self.IP = ImageNet(2048)\n",
    "\n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        temp_img = x[0]\n",
    "        batch_size = temp_img.shape[0]\n",
    "        #img = self.img_encoder(temp_img)\n",
    "        img = self.IP(temp_img)\n",
    "        img = img.view((batch_size, self.img_encoder_out_classes, 1, 1))\n",
    "        print(\"img.shape = \",img.shape)\n",
    "        \n",
    "        ques = x[1]\n",
    "        q_len = x[2]\n",
    "        ques = self.text(ques, list(q_len.data))\n",
    "        a = self.attention(img, ques)\n",
    "        img = apply_attention(img, a)\n",
    "        combined = torch.cat([img, ques], dim=1)\n",
    "        answer = self.classifier(combined)\n",
    "        #answer = self.softmax(combined)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main:\n",
    "#   experiment_name_prefix: my_exp\n",
    "#   seed: 1\n",
    "#   num_workers: 6\n",
    "#   parallel: True\n",
    "#   gpus_to_use: 0,1\n",
    "#   trains: False\n",
    "#   paths:\n",
    "#     train: 'data/train.pkl'\n",
    "#     validation: 'data/validation.pkl'\n",
    "#     logs: 'logs/'\n",
    "#     train_images: '../../../datashare/train2014'\n",
    "#     train_qeustions: '../../../datashare/v2_OpenEnded_mscoco_train2014_questions.json'\n",
    "#     train_answers: '../../../datashare/v2_mscoco_train2014_annotations.json'\n",
    "#     val_images: '../../../datashare/val2014'\n",
    "#     val_qeustions: '../../../datashare/v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "#     val_answers: '../../../datashare/v2_mscoco_val2014_annotations.json'    \n",
    " \n",
    "\n",
    "num_epochs = 50\n",
    "grad_clip = 0.0\n",
    "dropout = 0.2\n",
    "num_hid = 20\n",
    "batch_size = 64\n",
    "save_model = True\n",
    "img_encoder_out_classes = 2048\n",
    "img_encoder_batchnorm = True\n",
    "img_encoder_dropout = 0\n",
    "text_embedding_tokens = 15193\n",
    "text_embedding_features = 300\n",
    "text_lstm_features = 1024\n",
    "text_dropout = 0.4\n",
    "attention_mid_features = 512\n",
    "attention_glimpses = 2\n",
    "attention_dropout = 0.5\n",
    "classifier_dropout = 0.5 \n",
    "classifier_mid_features = 512\n",
    "classifier_out_classes = 2410\n",
    "#   lr:\n",
    "#     lr_value: 5e-4\n",
    "#     lr_decay: 5\n",
    "#     lr_gamma: 0.1\n",
    "#     lr_step_size: 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matansudry/miniconda3/envs/matan_env/lib/python3.7/site-packages/ipykernel_launcher.py:214: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/matansudry/miniconda3/envs/matan_env/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "# Init model\n",
    "image_in_size_input = ((3,224,224))\n",
    "img_encoder_channels_input = [64, 64, 64, 64, 64, 64, 64, 64,\n",
    "                              128, 128, 128, 128, 128, 128, 128, 128,\n",
    "                              256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
    "                              512, 512, 512, 512, 512]#, 1024]\n",
    "model = MyModel(image_in_size=image_in_size_input,\n",
    "    img_encoder_out_classes=img_encoder_out_classes,\n",
    "    img_encoder_channels=img_encoder_channels_input,\n",
    "    img_encoder_batchnorm=img_encoder_batchnorm,\n",
    "    img_encoder_dropout=img_encoder_dropout,\n",
    "    text_embedding_tokens=text_embedding_tokens,\n",
    "    text_embedding_features=text_embedding_features,\n",
    "    text_lstm_features=text_lstm_features,\n",
    "    text_dropout=text_dropout,\n",
    "    attention_mid_features=attention_mid_features,\n",
    "    attention_glimpses=attention_glimpses,\n",
    "    attention_dropout=attention_dropout,\n",
    "    classifier_dropout=classifier_dropout,\n",
    "    classifier_mid_features=classifier_mid_features,\n",
    "    classifier_out_classes=classifier_out_classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'MyModel' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-2ac0ad3860a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/matan_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    770\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 772\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'MyModel' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "print(model.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  (IP): ImageNet(\n",
    "    (model): ResNet(\n",
    "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "      (layer1): Sequential(\n",
    "        (0): BasicBlock(\n",
    "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "        (1): BasicBlock(\n",
    "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "        (2): BasicBlock(\n",
    "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (layer2): Sequential(\n",
    "        (0): BasicBlock(\n",
    "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (downsample): Sequential(\n",
    "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          )\n",
    "        )\n",
    "        (1): BasicBlock(\n",
    "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "        (2): BasicBlock(\n",
    "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "        (3): BasicBlock(\n",
    "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (layer3): Sequential(\n",
    "        (0): BasicBlock(\n",
    "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (downsample): Sequential(\n",
    "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          )\n",
    "        )\n",
    "        (1): BasicBlock(\n",
    "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "        (2): BasicBlock(\n",
    "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "        (3): BasicBlock(\n",
    "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "        (4): BasicBlock(\n",
    "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "        (5): BasicBlock(\n",
    "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (layer4): Sequential(\n",
    "        (0): BasicBlock(\n",
    "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (downsample): Sequential(\n",
    "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          )\n",
    "        )\n",
    "        (1): BasicBlock(\n",
    "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "        (2): BasicBlock(\n",
    "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "      (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/trainval_ans2label.pkl\", \"rb\") as f:\n",
    "    unpickler = pickle.Unpickler(f)\n",
    "    # if file is not empty scores will be equal\n",
    "    # to the value unpickled\n",
    "    dict_answers = unpickler.load()\n",
    "    number_of_answers_per_question = len(dict_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1073"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_answers_per_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "temp = torch.ones(64,1073)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64, 1073])\n"
     ]
    }
   ],
   "source": [
    "print(matan.shape)\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matan = temp.sum(dim=1)\n",
    "for i in range(len(matan)):\n",
    "    temp[i] = temp[i]/matan[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0009, 0.0009, 0.0009,  ..., 0.0009, 0.0009, 0.0009],\n",
       "        [0.0009, 0.0009, 0.0009,  ..., 0.0009, 0.0009, 0.0009],\n",
       "        [0.0009, 0.0009, 0.0009,  ..., 0.0009, 0.0009, 0.0009],\n",
       "        ...,\n",
       "        [0.0009, 0.0009, 0.0009,  ..., 0.0009, 0.0009, 0.0009],\n",
       "        [0.0009, 0.0009, 0.0009,  ..., 0.0009, 0.0009, 0.0009],\n",
       "        [0.0009, 0.0009, 0.0009,  ..., 0.0009, 0.0009, 0.0009]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matan_env",
   "language": "python",
   "name": "matan_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
