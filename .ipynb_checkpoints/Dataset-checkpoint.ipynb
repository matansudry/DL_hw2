{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here, we create a custom dataset\n",
    "\"\"\"\n",
    "import torch\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import utils\n",
    "import tqdm\n",
    "from utils.types import PathT\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Any, Tuple, Dict, List\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from models.base_model import MyModel\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "# from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoImages(data.Dataset):\n",
    "    \"\"\" Dataset for MSCOCO images located in a folder on the filesystem \"\"\"\n",
    "    def __init__(self, path, transform=None):\n",
    "        super(CocoImages, self).__init__()\n",
    "        self.path = path\n",
    "        self.id_to_filename = self._find_images()\n",
    "        self.sorted_ids = sorted(self.id_to_filename.keys())  # used for deterministic iteration order\n",
    "        print('found {} images in {}'.format(len(self), self.path))\n",
    "        self.transform = transform\n",
    "\n",
    "    def _find_images(self):\n",
    "        id_to_filename = {}\n",
    "        for filename in os.listdir(self.path):\n",
    "            if not filename.endswith('.jpg'):\n",
    "                continue\n",
    "            id_and_extension = filename.split('_')[-1]\n",
    "            id = int(id_and_extension.split('.')[0])\n",
    "            id_to_filename[id] = filename\n",
    "        return id_to_filename\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        id = self.sorted_ids[item]\n",
    "        path = os.path.join(self.path, self.id_to_filename[id])\n",
    "        img = Image.open(path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return id, img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sorted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset template. Implement the empty functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_path, questions_path, answers_path, train=True, answerable_only=False):#, answerable_only=False):\n",
    "        # Set variables\n",
    "        self.image_features_path = image_path\n",
    "        self.questions_path = questions_path\n",
    "        self.answers_path = answers_path\n",
    "        \n",
    "        #load the dataset of I, Q, A including the vocab of Q and A\n",
    "        with open(questions_path, 'r') as fd:\n",
    "            self.questions_json = json.load(fd)\n",
    "            \n",
    "        if (train):\n",
    "            dataset_type = \"train\"\n",
    "        else:\n",
    "            dataset_type = \"val\"\n",
    "            \n",
    "        #load question vocab\n",
    "        with open(\"../data/cache/question_vocab_\"+dataset_type, 'r') as fd:\n",
    "            vocab_json = json.load(fd)\n",
    "        \n",
    "\n",
    "        \n",
    "        #Vocab\n",
    "        self.vocab = vocab_json\n",
    "        self.token_to_index = self.vocab#['question']\n",
    "        \n",
    "        with open(\"../data/cache/trainval_ans2label.pkl\", \"rb\") as f:\n",
    "            unpickler = pickle.Unpickler(f)\n",
    "            # if file is not empty scores will be equal\n",
    "            # to the value unpickled\n",
    "            dict_answers = unpickler.load()\n",
    "            self.number_of_answers_per_question = len(dict_answers)\n",
    "        \n",
    "        \n",
    "        print(\"files upload was done\")\n",
    "        \n",
    "        #load Q\n",
    "        if os.path.isfile(\"../data/questions_\"+dataset_type):\n",
    "            self.questions = torch.load(\"../data/questions_\"+dataset_type)\n",
    "        else:\n",
    "            self.questions = list(self.prepare_questions())\n",
    "            self.questions = [self._encode_question(q, self.token_to_index) for q in self.questions] \n",
    "            torch.save(self.questions, \"../data/questions_\"+dataset_type)\n",
    "        \n",
    "        print(\"questions done\")\n",
    "        \n",
    "        #change Q to Q dict    \n",
    "        if os.path.isfile(\"../data/questions_dict_\"+dataset_type):\n",
    "            with open(\"../data/questions_dict_\"+dataset_type, 'rb') as handle:\n",
    "                self.questions_dict = pickle.load(handle)\n",
    "        else:\n",
    "            self.questions_dict = self.questions_to_dict()\n",
    "            with open(\"../data/questions_dict_\"+dataset_type, 'wb') as handle:\n",
    "                pickle.dump(self.questions_dict, handle)\n",
    "                \n",
    "        print(\"questions dict done\")\n",
    "                \n",
    "        #Load question_id_to_image_id\n",
    "        if os.path.isfile(\"../data/question_id_to_image_id_\"+dataset_type):\n",
    "            with open(\"../data/question_id_to_image_id_\"+dataset_type, 'r') as fd:\n",
    "                self.question_id_to_image_id = json.load(fd)\n",
    "        else:\n",
    "            self.question_id_to_image_id = self.question_id_to_image_id()\n",
    "            with open(\"../data/question_id_to_image_id_\"+dataset_type, 'w') as fd:\n",
    "                json.dump(self.question_id_to_image_id, fd)\n",
    "\n",
    "        print(\"question_id_to_image_id done\")\n",
    "        \n",
    "        \n",
    "        #load A\n",
    "        self.answerable_only = answerable_only\n",
    "        if os.path.isfile(\"../data/answerable_with_labels_only_\"+dataset_type+\"_\"+str(answerable_only)):\n",
    "            with open(\"../data/answerable_with_labels_only_\"+dataset_type+\"_\"+str(answerable_only), 'rb') as handle:\n",
    "                self.answerable = pickle.load(handle)\n",
    "        else:\n",
    "            #preprocess A\n",
    "            self.answerable = self.preprocess_answers(train)\n",
    "            if self.answerable_only:\n",
    "                self.answerable = self._find_answerable()\n",
    "            with open(\"../data/answerable_with_labels_only_\"+dataset_type+\"_\"+str(answerable_only), 'wb') as handle:\n",
    "                pickle.dump(self.answerable, handle)\n",
    "        \n",
    "        print(\"answers done\")\n",
    "        \n",
    "        #load I\n",
    "        if os.path.isfile(\"../data/images_\"+dataset_type):\n",
    "            with open(\"../data/images_\"+dataset_type, 'rb') as handle:\n",
    "                self.images = pickle.load(handle)\n",
    "\n",
    "        else:\n",
    "            #preprocess A\n",
    "            self.images = self.load_images()\n",
    "            with open(\"../data/images_\"+dataset_type, 'wb') as handle:\n",
    "                pickle.dump(self.images, handle)\n",
    "        \n",
    "        print(\"images done\")\n",
    "        \n",
    "        #load coco_images_to_dict\n",
    "        if os.path.isfile(\"../data/coco_images_to_dict\"+dataset_type):\n",
    "            with open(\"../data/coco_images_to_dict\"+dataset_type, 'rb') as handle:\n",
    "                self.images_dict = pickle.load(handle)\n",
    "\n",
    "        else:\n",
    "            self.coco_images_to_dict()\n",
    "            with open(\"../data/coco_images_to_dict\"+dataset_type, 'wb') as handle:\n",
    "                pickle.dump(self.images_dict, handle)\n",
    "                \n",
    "        print(\"coco_images_to_dict done\")\n",
    "                \n",
    "        self.index_to_question_number_dict = self.index_to_question_number_func()\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        item = self.index_to_question_number_dict[item]\n",
    "        q, q_length = self.questions_dict[item]\n",
    "        a = self.answerable[item]\n",
    "        temp = torch.zeros(self.number_of_answers_per_question)\n",
    "        for answer_index in range(len(a[0])):\n",
    "            temp[a[0][answer_index]] = a[1][answer_index]\n",
    "        image_id = self.question_id_to_image_id[str(item)]\n",
    "        image_id = self.images_dict[image_id]\n",
    "        v = self.images[0][image_id][1]\n",
    "        \n",
    "        return v, temp, q, item, q_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        :return: the length of the dataset (number of sample).\n",
    "        \"\"\"\n",
    "        return len(self.questions_dict)\n",
    "    \n",
    "    def get_transform(self, target_size, central_fraction=1.0):\n",
    "        return transforms.Compose([\n",
    "            transforms.Scale(int(target_size / central_fraction)),\n",
    "            transforms.CenterCrop(target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    def load_images(self):\n",
    "        transform = self.get_transform(target_size=224, central_fraction=0.875)\n",
    "        dataset = [CocoImages(self.image_features_path, transform=transform)]\n",
    "    #     dataset = Composite(dataset)\n",
    "        return dataset\n",
    "    \n",
    "    @property\n",
    "    def max_question_length(self):\n",
    "        if not hasattr(self, '_max_length'):\n",
    "            self._max_length = max(map(len, self.questions))\n",
    "        return self._max_length\n",
    "                    \n",
    "    def preprocess_answers(self, train=True):\n",
    "        if train:\n",
    "            with open(\"../data/cache/train_target.pkl\", \"rb\") as f:\n",
    "                unpickler = pickle.Unpickler(f)\n",
    "                scores = unpickler.load()\n",
    "        else:\n",
    "            with open(\"../data/cache/val_target.pkl\", \"rb\") as f:\n",
    "                unpickler = pickle.Unpickler(f)\n",
    "                scores = unpickler.load()  \n",
    "\n",
    "        with open(\"../data/cache/trainval_ans2label.pkl\", \"rb\") as f:\n",
    "            unpickler = pickle.Unpickler(f)\n",
    "            # if file is not empty scores will be equal\n",
    "            # to the value unpickled\n",
    "            dict_answers = unpickler.load()\n",
    "            self.number_of_answers_per_question = len(dict_answers)\n",
    "\n",
    "        answers_dict = {}\n",
    "        for item in scores:\n",
    "            answers_dict[item['question_id']] = ((item['labels'], item['scores']))\n",
    "\n",
    "        return answers_dict\n",
    "    \n",
    "    def questions_to_dict(self):\n",
    "        question_dict = {}\n",
    "        for i in range(len(self.questions_json['questions'])):\n",
    "            question_dict[self.questions_json['questions'][i]['question_id']] = self.questions[i] \n",
    "        return (question_dict)\n",
    "    \n",
    "    def question_id_to_image_id(self):\n",
    "        question_id_dict = {}\n",
    "        for i in range(len(self.questions_json['questions'])):\n",
    "            question_id_dict[str(self.questions_json['questions'][i]['question_id'])] = self.questions_json['questions'][i]['image_id']\n",
    "        return (question_id_dict)\n",
    "    \n",
    "    def _find_answerable(self):\n",
    "        update_answers = self.answerable.copy()\n",
    "        for answer in tqdm.tqdm(self.answerable):\n",
    "            if (sum(self.answerable[answer])==0):\n",
    "                del update_answers[answer]\n",
    "        return(update_answers)\n",
    "               \n",
    "    def prepare_questions(self):\n",
    "        \"\"\" Tokenize and normalize questions from a given question json in the usual VQA format. \"\"\"\n",
    "        questions = [q['question'] for q in self.questions_json['questions']]\n",
    "        for question in questions:\n",
    "            question = question.lower()[:-1]\n",
    "            yield question.split(' ')\n",
    "\n",
    "    def _encode_question(self, question, token_to_index):\n",
    "        \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n",
    "        vec = torch.zeros(self.max_question_length).long()\n",
    "        for i, token in enumerate(question):\n",
    "            index = token_to_index.get(token, 0)\n",
    "            vec[i] = index\n",
    "        return vec, len(question)\n",
    "\n",
    "    def index_to_question_number_func(self):\n",
    "        index_to_question_number_dict = {}\n",
    "        cnt = 0\n",
    "        for question in self.answerable:\n",
    "            index_to_question_number_dict[cnt] = question\n",
    "            cnt += 1\n",
    "        return index_to_question_number_dict\n",
    "    \n",
    "    def coco_images_to_dict(self):\n",
    "        images_dict= {}\n",
    "        images = self.images[0]\n",
    "        cnt = 0\n",
    "        for image in tqdm.tqdm(images):\n",
    "            images_dict[image[0]] = cnt\n",
    "            cnt +=1\n",
    "        self.images_dict = images_dict\n",
    "    def num_tokens(self):\n",
    "        return len(self.vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files upload was done\n",
      "questions done\n",
      "questions dict done\n",
      "question_id_to_image_id done\n",
      "answers done\n",
      "images done\n",
      "coco_images_to_dict done\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = MyDataset(image_path='../../../datashare/train2014',\n",
    "#                           questions_path='../../../datashare/v2_OpenEnded_mscoco_train2014_questions.json',\n",
    "#                           answers_path='../../../datashare/v2_mscoco_train2014_annotations.json',\n",
    "#                           train=True,\n",
    "#                           answerable_only = False\n",
    "#                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files upload was done\n",
      "questions done\n",
      "questions dict done\n",
      "question_id_to_image_id done\n",
      "answers done\n",
      "images done\n",
      "coco_images_to_dict done\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = MyDataset(image_path='../../../datashare/val2014',\n",
    "#                           questions_path='../../../datashare/v2_OpenEnded_mscoco_val2014_questions.json',\n",
    "#                           answers_path='../../../datashare/v2_mscoco_val2014_annotations.json',\n",
    "#                           train=False,\n",
    "#                           answerable_only = False\n",
    "#                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, 2, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/DL_hw2/models/base_model.py:213: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(w)\n",
      "/home/student/DL_hw2/models/base_model.py:209: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.embedding.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out =  tensor([[-0.0300, -0.3011, -0.0043,  ..., -0.3843,  0.2587, -0.4227],\n",
      "        [ 0.1914, -0.0346, -0.4854,  ..., -0.2546,  0.6380,  0.2582]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "out.shape =  torch.Size([2, 2410])\n",
      "ans.shape =  torch.Size([2, 2410])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-628396f91bfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"out.shape = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ans.shape = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "from models.base_model import MyModel\n",
    "model = MyModel()\n",
    "# model = torch.nn.DataParallel(model)\n",
    "model = model.cuda()\n",
    "for i, img, ans, ques, _, q_len in train_loader:\n",
    "    img = img.cuda()\n",
    "    ans = ans.cuda()\n",
    "    ques = ques.cuda()\n",
    "    q_len = q_len.cuda()\n",
    "    out = model((img, ques, q_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS_env",
   "language": "python",
   "name": "cs_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
