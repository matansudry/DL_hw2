{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "import config\n",
    "import data\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_questions(questions_json):\n",
    "    \"\"\" Tokenize and normalize questions from a given question json in the usual VQA format. \"\"\"\n",
    "    questions = [q['question'] for q in questions_json['questions']]\n",
    "    for question in questions:\n",
    "        question = question.lower()[:-1]\n",
    "        yield question.split(' ')\n",
    "\n",
    "def extract_vocab(iterable, top_k=None, start=0):\n",
    "    \"\"\" Turns an iterable of list of tokens into a vocabulary.\n",
    "        These tokens could be single answers or word tokens in questions.\n",
    "    \"\"\"\n",
    "    all_tokens = itertools.chain.from_iterable(iterable)\n",
    "    counter = Counter(all_tokens)\n",
    "    if top_k:\n",
    "        most_common = counter.most_common(top_k)\n",
    "        most_common = (t for t, c in most_common)\n",
    "    else:\n",
    "        most_common = counter.keys()\n",
    "    # descending in count, then lexicographical order\n",
    "    tokens = sorted(most_common, key=lambda x: (counter[x], x), reverse=True)\n",
    "    vocab = {t: i for i, t in enumerate(tokens, start=start)}\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def main():\n",
    "    questions = \"data/v2_OpenEnded_mscoco_val2014_questions.json\"\n",
    "#     answers = \"v2_mscoco_train2014_annotations.json\"\n",
    "\n",
    "    with open(questions, 'r') as fd:\n",
    "        questions = json.load(fd)\n",
    "#     with open(answers, 'r') as fd:\n",
    "#         answers = json.load(fd)\n",
    "\n",
    "    questions = prepare_questions(questions)\n",
    "#     answers = data.prepare_answers(answers)\n",
    "\n",
    "    question_vocab = extract_vocab(questions, start=1)\n",
    "#     answer_vocab = extract_vocab(answers, top_k=config.max_answers)\n",
    "\n",
    "#     vocabs = {\n",
    "#         'question': question_vocab,\n",
    "#         'answer': answer_vocab,\n",
    "#     }\n",
    "    with open(\"data/cache/question_vocab_val\", 'w') as fd:\n",
    "        json.dump(question_vocab, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matan_env",
   "language": "python",
   "name": "matan_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
