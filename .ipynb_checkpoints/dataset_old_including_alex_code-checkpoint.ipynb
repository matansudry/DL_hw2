{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here, we create a custom dataset\n",
    "\"\"\"\n",
    "import torch\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import utils\n",
    "from utils.types import PathT\n",
    "from torch.utils.data as data\n",
    "from typing import Any, Tuple, Dict, List\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "# from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset template. Implement the empty functions.\n",
    "    \"\"\"\n",
    "#     def __init__(self, path: PathT) -> None:\n",
    "#         # Set variables\n",
    "#         self.path = path\n",
    "\n",
    "#         # Load features\n",
    "#         self.features = self._get_features()\n",
    "\n",
    "#         # Create list of entries\n",
    "#         self.entries = self._get_entries()\n",
    "\n",
    "# #     def __getitem__(self, index: int) -> Tuple:\n",
    "# #         return self.entries[index]['x'], self.entries[index]['y']\n",
    "\n",
    "    def __init__(self, image_features_path, questions_path, answers_path, answerable_only=False):\n",
    "        super(VQA, self).__init__()\n",
    "        with open(questions_path, 'r') as fd:\n",
    "            questions_json = json.load(fd)\n",
    "        with open(answers_path, 'r') as fd:\n",
    "            answers_json = json.load(fd)\n",
    "        with open(\"vocab.json\", 'r') as fd: # maybe we should remove it\n",
    "            vocab_json = json.load(fd)\n",
    "        self._check_integrity(questions_json, answers_json)\n",
    "\n",
    "        # vocab\n",
    "        self.vocab = vocab_json\n",
    "        self.token_to_index = self.vocab['question']\n",
    "        self.answer_to_index = self.vocab['answer']\n",
    "\n",
    "        # q and a\n",
    "        self.questions = list(prepare_questions(questions_json))\n",
    "        self.answers = list(prepare_answers(answers_json))\n",
    "        self.questions = [self._encode_question(q) for q in self.questions]\n",
    "        self.answers = [self._encode_answers(a) for a in self.answers]\n",
    "\n",
    "        # v\n",
    "        self.image_features_path = image_features_path\n",
    "        self.coco_id_to_index = self._create_coco_id_to_index()\n",
    "        self.coco_ids = [q['image_id'] for q in questions_json['questions']]\n",
    "\n",
    "        # only use questions that have at least one answer?\n",
    "        self.answerable_only = answerable_only\n",
    "        if self.answerable_only:\n",
    "            self.answerable = self._find_answerable()\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.answerable_only:\n",
    "            # change of indices to only address answerable questions\n",
    "            item = self.answerable[item]\n",
    "\n",
    "        q, q_length = self.questions[item]\n",
    "        a = self.answers[item]\n",
    "        image_id = self.coco_ids[item]\n",
    "        v = self._load_image(image_id)\n",
    "        # since batches are re-ordered for PackedSequence's, the original question order is lost\n",
    "        # we return `item` so that the order of (v, q, a) triples can be restored if desired\n",
    "        # without shuffling in the dataloader, these will be in the order that they appear in the q and a json's.\n",
    "        return v, q, a, item, q_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        :return: the length of the dataset (number of sample).\n",
    "        \"\"\"\n",
    "        return len(self.entries)\n",
    "\n",
    "    def _get_features(self) -> Any:\n",
    "        \"\"\"\n",
    "        Load all features into a structure (not necessarily dictionary). Think if you need/can load all the features\n",
    "        into the memory.\n",
    "        :return:\n",
    "        :rtype:\n",
    "        \"\"\"\n",
    "        with open(self.path, \"rb\") as features_file:\n",
    "            features = pickle.load(features_file)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _get_entries(self) -> List:\n",
    "        \"\"\"\n",
    "        This function create a list of all the entries. We will use it later in __getitem__\n",
    "        :return: list of samples\n",
    "        \"\"\"\n",
    "        entries = []\n",
    "\n",
    "        for idx, item in self.features.items():\n",
    "            entries.append(self._get_entry(item))\n",
    "\n",
    "        return entries\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_entry(item: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        :item: item from the data. In this example, {'input': Tensor, 'y': int}\n",
    "        \"\"\"\n",
    "        x = item['input']\n",
    "        y = torch.Tensor([1, 0]) if item['label'] else torch.Tensor([0, 1])\n",
    "\n",
    "        return {'x': x, 'y': y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class CocoImages(data.Dataset):\n",
    "    \"\"\" Dataset for MSCOCO images located in a folder on the filesystem \"\"\"\n",
    "    def __init__(self, path, transform=None):\n",
    "        super(CocoImages, self).__init__()\n",
    "        self.path = path\n",
    "        self.id_to_filename = self._find_images()\n",
    "        self.sorted_ids = sorted(self.id_to_filename.keys())  # used for deterministic iteration order\n",
    "        print('found {} images in {}'.format(len(self), self.path))\n",
    "        self.transform = transform\n",
    "\n",
    "    def _find_images(self):\n",
    "        id_to_filename = {}\n",
    "        for filename in os.listdir(self.path):\n",
    "            if not filename.endswith('.jpg'):\n",
    "                continue\n",
    "            id_and_extension = filename.split('_')[-1]\n",
    "            id = int(id_and_extension.split('.')[0])\n",
    "            id_to_filename[id] = filename\n",
    "        return id_to_filename\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        id = self.sorted_ids[item]\n",
    "        path = os.path.join(self.path, self.id_to_filename[id])\n",
    "        img = Image.open(path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return id, img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sorted_ids)\n",
    "    \n",
    "    class Composite(data.Dataset):\n",
    "    \"\"\" Dataset that is a composite of several Dataset objects. Useful for combining splits of a dataset. \"\"\"\n",
    "    def __init__(self, datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        current = self.datasets\n",
    "        for d in self.datasets:\n",
    "            if item < len(d):\n",
    "                return d[item]\n",
    "            item -= len(d)\n",
    "        else:\n",
    "            raise IndexError('Index too large for composite dataset')\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(map(len, self.datasets))\n",
    "    \n",
    "    def get_transform(target_size, central_fraction=1.0):\n",
    "    return transforms.Compose([\n",
    "        transforms.Scale(int(target_size / central_fraction)),\n",
    "        transforms.CenterCrop(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = get_transform(224, 0.875)\n",
    "paths = [\"data/train2014\", \"data/val2014\"]\n",
    "# temp = CocoImages(\"data/train2014\", transform=transform)\n",
    "train_dataset = [CocoImages(\"data/train2014\", transform=transform)]\n",
    "val_dataset = [CocoImages(\"data/val2014\", transform=transform)]\n",
    "train_dataset = Composite(train_dataset)\n",
    "val_dataset = Composite(val_dataset)\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "                          \n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = MyDataset('data/train.pkl')\n",
    "train_dataset = MyDataset(\"data/train2014\")\n",
    "val_dataset = MyDataset('data/validation.pkl')\n",
    "train_loader = DataLoader(train_dataset, 16 shuffle=True,\n",
    "                          num_workers=1)\n",
    "eval_loader = DataLoader(val_dataset, 16 shuffle=True,\n",
    "                         num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for img, ques, ans, q_len in tq:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matan_env",
   "language": "python",
   "name": "matan_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
