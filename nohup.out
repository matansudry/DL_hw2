/home/student/.conda/envs/cs236781-hw/lib/python3.8/site-packages/torchvision/transforms/transforms.py:256: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.
  warnings.warn("The use of the transforms.Scale transform is deprecated, " +
[2021-01-08 16:04:49,362] [TRAIN] - main:
  experiment_name_prefix: my_exp
  seed: 1
  num_workers: 6
  parallel: true
  gpus_to_use: 0,1
  trains: false
  paths:
    train: data/train.pkl
    validation: data/validation.pkl
    logs: logs/
    train_images: ../../../datashare/train2014
    train_qeustions: ../../../datashare/v2_OpenEnded_mscoco_train2014_questions.json
    train_answers: ../../../datashare/v2_mscoco_train2014_annotations.json
    val_images: ../../../datashare/val2014
    val_qeustions: ../../../datashare/v2_OpenEnded_mscoco_val2014_questions.json
    val_answers: ../../../datashare/v2_mscoco_val2014_annotations.json
train:
  num_epochs: 30
  grad_clip: 0.0
  dropout: 0.2
  num_hid: 20
  batch_size: 64
  save_model: true
  img_encoder_out_classes: 4096
  img_encoder_batchnorm: true
  img_encoder_dropout: 0.0
  text_embedding_tokens: 13659
  text_embedding_features: 300
  text_lstm_features: 4096
  text_dropout: 0.4
  classifier_dropout: 0.5
  classifier_mid_features: 512
  classifier_out_classes: 2410
  lr:
    lr_value: 0.001

/home/student/DL_hw2/models/base_model.py:193: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  init.xavier_uniform(w)
/home/student/DL_hw2/models/base_model.py:189: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  init.xavier_uniform(self.embedding.weight)
[2021-01-08 16:06:42,549] [TRAIN] - DataParallel(
  (module): MyModel(
    (dropout): Dropout(p=0.5, inplace=False)
    (fc1): Linear(in_features=4096, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=2410, bias=True)
    (relu): ReLU(inplace=True)
    (img_encoder): ResNetClassifier(
      (feature_extractor): Sequential(
        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        (4): ResidualBlock(
          (main_path): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (5): ResidualBlock(
          (main_path): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (6): ResidualBlock(
          (main_path): Sequential(
            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (7): ResidualBlock(
          (main_path): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (8): ResidualBlock(
          (main_path): Sequential(
            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (9): ResidualBlock(
          (main_path): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (10): ResidualBlock(
          (main_path): Sequential(
            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (11): AdaptiveAvgPool2d(output_size=(1, 1))
      )
      (fc1): Linear(in_features=512, out_features=1000, bias=True)
      (fc2): Linear(in_features=1000, out_features=4096, bias=True)
      (relu): ReLU(inplace=True)
    )
    (text): TextProcessor(
      (embedding): Embedding(13659, 300, padding_idx=0)
      (drop): Dropout(p=0.4, inplace=False)
      (tanh): Tanh()
      (lstm): LSTM(300, 1024, num_layers=2)
      (fc): Linear(in_features=1024, out_features=4096, bias=True)
    )
  )
)
Params: 32444342
  0%|          | 0/30 [00:00<?, ?it/s]current dict =  /home/student/DL_hw2/logs/hydra
<utils.train_logger.TrainLogger object at 0x7fe11342cd60>
files upload was done
opening existing Qs
questions done
question_id_to_image_id done
answers done
images done
files upload was done
opening existing Qs
questions done
question_id_to_image_id done
answers done
images done
/home/student/DL_hw2/train.py:76: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  nll = -F.log_softmax(y_hat)
[2021-01-08 17:41:39,442] [TRAIN] - (EPOCH 0) Time: 5696.89, Gradient norm: 1.5235, Train loss: 3.92, Train Score: 28.14, Val score: 30.97
  3%|â–Ž         | 1/30 [1:34:57<45:53:50, 5697.59s/it]val loss =  tensor(0.0531, device='cuda:0')
