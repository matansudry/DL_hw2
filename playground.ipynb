{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import utils\n",
    "import torch\n",
    "import pickle\n",
    "import pprint\n",
    "import tqdm\n",
    "from utils.types import PathT\n",
    "# from torch.utils.data import Dataset\n",
    "import torch.utils.data as data\n",
    "from typing import Any, Tuple, Dict, List\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoImages(data.Dataset):\n",
    "    \"\"\" Dataset for MSCOCO images located in a folder on the filesystem \"\"\"\n",
    "    def __init__(self, path, transform=None):\n",
    "        super(CocoImages, self).__init__()\n",
    "        self.path = path\n",
    "        self.id_to_filename = self._find_images()\n",
    "        self.sorted_ids = sorted(self.id_to_filename.keys())  # used for deterministic iteration order\n",
    "        print('found {} images in {}'.format(len(self), self.path))\n",
    "        self.transform = transform\n",
    "\n",
    "    def _find_images(self):\n",
    "        id_to_filename = {}\n",
    "        for filename in os.listdir(self.path):\n",
    "            if not filename.endswith('.jpg'):\n",
    "                continue\n",
    "            id_and_extension = filename.split('_')[-1]\n",
    "            id = int(id_and_extension.split('.')[0])\n",
    "            id_to_filename[id] = filename\n",
    "        return id_to_filename\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        id = self.sorted_ids[item]\n",
    "        path = os.path.join(self.path, self.id_to_filename[id])\n",
    "        img = Image.open(path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return id, img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sorted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Composite(data.Dataset):\n",
    "    \"\"\" Dataset that is a composite of several Dataset objects. Useful for combining splits of a dataset. \"\"\"\n",
    "    def __init__(self, datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        current = self.datasets\n",
    "        for d in self.datasets:\n",
    "            if item < len(d):\n",
    "                return d[item]\n",
    "            item -= len(d)\n",
    "        else:\n",
    "            raise IndexError('Index too large for composite dataset')\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(map(len, self.datasets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(target_size, central_fraction=1.0):\n",
    "    return transforms.Compose([\n",
    "        transforms.Scale(int(target_size / central_fraction)),\n",
    "        transforms.CenterCrop(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matansudry/miniconda3/envs/matan_env/lib/python3.7/site-packages/torchvision/transforms/transforms.py:256: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 82783 images in data/train2014\n",
      "found 40504 images in data/val2014\n"
     ]
    }
   ],
   "source": [
    "transform = get_transform(224, 0.875)\n",
    "paths = [\"data/train2014\", \"data/val2014\"]\n",
    "# temp = CocoImages(\"data/train2014\", transform=transform)\n",
    "train_dataset = [CocoImages(\"data/train2014\", transform=transform)]\n",
    "val_dataset = [CocoImages(\"data/val2014\", transform=transform)]\n",
    "# train_dataset = Composite(train_dataset)\n",
    "val_dataset = Composite(val_dataset)\n",
    "# train_data_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=16,\n",
    "#     num_workers=8,\n",
    "#     shuffle=False,\n",
    "#     pin_memory=True,\n",
    "# )\n",
    "                          \n",
    "# val_data_loader = torch.utils.data.DataLoader(\n",
    "#     val_dataset,\n",
    "#     batch_size=16,\n",
    "#     num_workers=0,\n",
    "#     shuffle=False,\n",
    "#     pin_memory=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image in enumerate(train_dataset):\n",
    "    print(\"i[0] = \", i)\n",
    "    print(\"image = \", image)\n",
    "    print(\"image[0] = \", image[0])\n",
    "    if i == 1:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/v2_OpenEnded_mscoco_train2014_questions.json\"\n",
    "with open(path, 'r') as q:\n",
    "    pprint.pprint(json.load(q))\n",
    "pprint.pprint(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_questions(questions_json):\n",
    "    \"\"\" Tokenize and normalize questions from a given question json in the usual VQA format. \"\"\"\n",
    "    questions = [q['question'] for q in questions_json['questions']]\n",
    "    for question in questions:\n",
    "        question = question.lower()[:-1]\n",
    "        yield question.split(' ')\n",
    "        \n",
    "def _encode_question(question, token_to_index):\n",
    "    \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n",
    "    vec = torch.zeros(max_question_length(question)).long()\n",
    "    for i, token in enumerate(question):\n",
    "        index = token_to_index.get(token, 0)\n",
    "        vec[i] = index\n",
    "    return vec, len(question)\n",
    "\n",
    "def max_question_length(self):\n",
    "    if not hasattr(self, '_max_length'):\n",
    "        _max_length = max(map(len, questions))\n",
    "    return _max_length\n",
    "\n",
    "def questions_to_dict(questions, questions_json):\n",
    "    question_dict = {}\n",
    "    for i in range(len(questions_json['questions'])):\n",
    "        question_dict[questions_json['questions'][i]['question_id']] = questions[i] \n",
    "    return (question_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/v2_OpenEnded_mscoco_train2014_questions.json\", 'r') as fd:\n",
    "    questions_json = json.load(fd)\n",
    "with open(\"data/cache/question_vocab\", 'r') as fd:\n",
    "    vocab_json = json.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab_json\n",
    "token_to_index = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 458752,\n",
       " 'question': 'What is this photo taken looking through?',\n",
       " 'question_id': 458752000}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_json['questions'][0]['question_id']\n",
    "# {'image_id': 458752,\n",
    "#    'question': 'What position is this man playing?',\n",
    "#    'question_id': 458752001},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list(prepare_questions(questions_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [_encode_question(questions[0], token_to_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [_encode_question(q, token_to_index) for q in tqdm.tqdm(questions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([  3,   2,   5,  27,  69,  98, 471,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0]), 7)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def questions_to_dict(questions, questions_json):\n",
    "    question_dict = {}\n",
    "    for i in range(len(questions_json['questions'])):\n",
    "        question_dict[questions_json['questions'][i]['question_id']] = questions[i]\n",
    "    return (question_dict)\n",
    "\n",
    "temp = questions_to_dict(questions, questions_json)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/questions_dict_train\", 'rb') as handle:\n",
    "    questions_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/question_id_to_image_id_train\", 'r') as fd:\n",
    "    question_id_to_image_id = json.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458752"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_id_to_image_id['458752000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matan_env",
   "language": "python",
   "name": "matan_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
